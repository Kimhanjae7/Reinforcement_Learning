import gym
import numpy as np

if not hasattr(np, 'bool'):
    np.bool = bool  # numpy.boolì´ ì—†ìœ¼ë©´ boolë¡œ ëŒ€ì²´

import torch
from stable_baselines3 import PPO

# CarRacing í™˜ê²½ ìƒì„± (í™”ë©´ ë Œë”ë§ ì„¤ì •)
env = gym.make("CarRacing-v2", render_mode="human")

# PPO ëª¨ë¸ ìƒì„± (CNN ì •ì±… ì‚¬ìš©)
model = PPO("CnnPolicy", env, verbose=1, tensorboard_log="./ppo_carracing_tensorboard/")

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
total_episodes = 100  # ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
timesteps_per_episode = 1000  # í•œ ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„

# í•™ìŠµ ì‹œì‘
print("ğŸ”¹ Training started...")
for episode in range(total_episodes):
    print(f"ğŸ”¹ Episode {episode+1} started")  # ì—í”¼ì†Œë“œ ì‹œì‘ ë¡œê·¸
    obs, _ = env.reset()
    total_reward = 0  # ì—í”¼ì†Œë“œ ë™ì•ˆ ë°›ì€ ì´ ë³´ìƒ

    for step in range(timesteps_per_episode):
        env.render()  # í™”ë©´ ì¶œë ¥

        # PPO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  í–‰ë™ ì„ íƒ
        action, _ = model.predict(obs)

        # í™˜ê²½ ì‹¤í–‰ ë° ë³´ìƒ íšë“
        obs, reward, done, truncated, _ = env.step(action)
        total_reward += reward  # ë³´ìƒ ëˆ„ì 

        if done or truncated:
            break  # ì—í”¼ì†Œë“œ ì¢…ë£Œ

    # ì—í”¼ì†Œë“œ ê²°ê³¼ ì¶œë ¥
    print(f"   â†’  Episode {episode+1} finished - Score: {total_reward:.2f}")

# í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥
model.save("ppo_carracing_model")
print("âœ… Training finished! Model saved as 'ppo_carracing_model'.")

# í•™ìŠµëœ ëª¨ë¸ ì‹¤í–‰ (ìë™ì°¨ ì£¼í–‰)
print("\nğŸ”¹ Running trained model...\n")
obs, _ = env.reset()
while True:
    env.render()
    action, _ = model.predict(obs)  # ìµœì  í–‰ë™ ì˜ˆì¸¡
    obs, reward, done, truncated, _ = env.step(action)
    if done or truncated:
        obs, _ = env.reset()  # ë‹¤ì‹œ ì‹œì‘
