import gym
import numpy as np
import random
import tensorflow as tf
from tensorflow import keras
from collections import deque


if not hasattr(np, 'bool'):
    np.bool = bool  # np.boolì´ ì—†ìœ¼ë©´ boolë¡œ ëŒ€ì²´

# ììœ¨ì£¼í–‰ í™˜ê²½ ì„¤ì • (CarRacing í™˜ê²½ ì‚¬ìš©)
env = gym.make("CarRacing-v2", render_mode="human")

# ìƒíƒœì™€ í–‰ë™ ê°œìˆ˜ ì •ì˜
state_size = env.observation_space.shape
action_size = env.action_space.shape[0]  # ì—°ì†í˜• í–‰ë™ ê³µê°„ì—ì„œëŠ” .shape[0] ì‚¬ìš©

# ê²½í—˜ ì €ì¥ì„ ìœ„í•œ ë²„í¼
memory = deque(maxlen=2000)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
gamma = 0.95        # í• ì¸ìœ¨ (ë¯¸ë˜ ë³´ìƒ ë°˜ì˜)
epsilon = 1.0       # íƒí—˜ë¥  (ëœë¤ í–‰ë™ ë¹„ìœ¨)
epsilon_min = 0.01  # íƒí—˜ ìµœì†Œê°’
epsilon_decay = 0.995  # íƒí—˜ë¥  ê°ì†Œìœ¨
batch_size = 32     # í•™ìŠµí•  ë°ì´í„° ê°œìˆ˜

# DQN ëª¨ë¸ ì •ì˜
def build_model():
    model = keras.Sequential([
        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=state_size),
        keras.layers.MaxPooling2D((2, 2)),
        keras.layers.Flatten(),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(action_size, activation='linear')  # ì—°ì†í˜• í–‰ë™ ê³µê°„ì—ì„œëŠ” ì„ í˜• ì¶œë ¥ ì‚¬ìš©
    ])
    model.compile(loss="mse", optimizer=keras.optimizers.Adam(learning_rate=0.001))
    return model

# ëª¨ë¸ ìƒì„±
model = build_model()

# DQN í•™ìŠµ í•¨ìˆ˜
def train():
    global epsilon
    print("ğŸ”¹ Training started...", flush=True)

    for episode in range(1000):  # 1000ë²ˆ í•™ìŠµ
        print(f"ğŸ”¹ Episode {episode} started (epsilon: {epsilon:.4f})", flush=True)
        state, _ = env.reset()
        state = np.reshape(state, [1, *state_size])

        for time in range(500):  # ìµœëŒ€ 500 í”„ë ˆì„ ì‹¤í–‰
            # í–‰ë™ ì„ íƒ (íƒí—˜ vs. í™œìš©)
            if np.random.rand() <= epsilon:
                action = env.action_space.sample()  # ëœë¤ í–‰ë™ ì„ íƒ
            else:
                q_values = model.predict(state, verbose=0)
                action = np.tanh(q_values[0])  # í–‰ë™ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
                action = np.clip(action, -1, 1)  # ì•ˆì „í•œ ë²”ìœ„ ìœ ì§€

            # í–‰ë™ ìˆ˜í–‰ ë° ë³´ìƒ í™•ì¸
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            next_state = np.reshape(next_state, [1, *state_size])

            # ê²½í—˜ ì €ì¥
            memory.append((state, action, reward, next_state, done))
            state = next_state

            if done:
                print(f" Episode {episode} finished - Score: {time}, Epsilon: {epsilon:.4f}", flush=True)
                break

        # ê²½í—˜ í•™ìŠµ (Replay)
        if len(memory) > batch_size:
            minibatch = random.sample(memory, batch_size)
            for state, action, reward, next_state, done in minibatch:
                target = reward
                if not done:
                    target = reward + gamma * np.max(model.predict(next_state, verbose=0)[0])
                target_f = model.predict(state, verbose=0)
                target_f[0] = target  # ì—°ì†í˜• í–‰ë™ ê³µê°„ì—ì„œëŠ” ì „ì²´ ê°’ì„ ì—…ë°ì´íŠ¸
                model.fit(state, target_f, epochs=1, verbose=0)

        # íƒí—˜ë¥  ê°ì†Œ
        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

# í•™ìŠµ ì‹œì‘
if __name__ == "__main__":
    train()
